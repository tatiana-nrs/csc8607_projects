{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preprocess] STRICT mode: RGB -> ToTensor -> Normalize\n",
      "[preprocess] mean: [0.48024, 0.44806, 0.3975]\n",
      "[preprocess] std : [0.27643, 0.26887, 0.28159]\n",
      "meta = {'num_classes': 200, 'input_shape': (3, 64, 64), 'seed': 42, 'sizes': {'train': 90000, 'val': 10000, 'test': 10000}}\n"
     ]
    }
   ],
   "source": [
    "import os, sys, yaml\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def find_repo_root(start=None):\n",
    "    \"\"\"Remonte les dossiers jusqu'à trouver un dossier 'src'.\"\"\"\n",
    "    if start is None:\n",
    "        start = os.getcwd()\n",
    "    cur = os.path.abspath(start)\n",
    "    while True:\n",
    "        if os.path.isdir(os.path.join(cur, \"src\")):\n",
    "            return cur\n",
    "        parent = os.path.dirname(cur)\n",
    "        if parent == cur:\n",
    "            raise RuntimeError(\"Impossible de trouver la racine du repo (dossier 'src').\")\n",
    "        cur = parent\n",
    "\n",
    "ROOT = find_repo_root()\n",
    "sys.path.insert(0, ROOT)\n",
    "\n",
    "cfg = yaml.safe_load(open(os.path.join(ROOT, \"configs\", \"config.yaml\"), \"r\"))\n",
    "\n",
    "from src.data_loading import get_dataloaders\n",
    "\n",
    "train_loader, val_loader, test_loader, meta = get_dataloaders(cfg)\n",
    "print(\"meta =\", meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF train size = 100000\n"
     ]
    }
   ],
   "source": [
    "def get_hf_split(torch_dataset):\n",
    "    \"\"\"\n",
    "    Essaie de retrouver le HF Dataset brut derrière ton dataset PyTorch wrapper.\n",
    "    Adapte la liste des attributs si besoin.\n",
    "    \"\"\"\n",
    "    for attr in (\"hf_ds\", \"base_ds\", \"ds\", \"dataset\"):\n",
    "        if hasattr(torch_dataset, attr):\n",
    "            return getattr(torch_dataset, attr)\n",
    "    raise AttributeError(\n",
    "        \"Je n'arrive pas à retrouver le HF Dataset brut derrière train_loader.dataset. \"\n",
    "        \"Ajoute un attribut (ex: self.hf_ds = hf_dataset) dans ton wrapper dataset.\"\n",
    "    )\n",
    "\n",
    "train_hf = get_hf_split(train_loader.dataset)\n",
    "val_hf   = get_hf_split(val_loader.dataset)\n",
    "test_hf  = get_hf_split(test_loader.dataset)\n",
    "\n",
    "print(\"HF train size =\", len(train_hf) + len(test_hf))  # = 100000 si ton split vient bien du train HF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[train (90k)]\n",
      "total = 90000\n",
      "modes: Counter({'RGB': 88353, 'L': 1647})\n",
      "sizes: Counter({(64, 64): 90000})\n",
      "nb images taille ≠ 64x64: 0\n",
      "nb labels non int: 0\n",
      "nb multi-label: 0\n",
      "\n",
      "[valid (10k)]\n",
      "total = 10000\n",
      "modes: Counter({'RGB': 9832, 'L': 168})\n",
      "sizes: Counter({(64, 64): 10000})\n",
      "nb images taille ≠ 64x64: 0\n",
      "nb labels non int: 0\n",
      "nb multi-label: 0\n",
      "\n",
      "[test (10k)]\n",
      "total = 10000\n",
      "modes: Counter({'RGB': 9826, 'L': 174})\n",
      "sizes: Counter({(64, 64): 10000})\n",
      "nb images taille ≠ 64x64: 0\n",
      "nb labels non int: 0\n",
      "nb multi-label: 0\n"
     ]
    }
   ],
   "source": [
    "def scan_split_hf(ds, split_name, expect_size=(64, 64), img_key=\"image\", label_key=\"label\"):\n",
    "    modes = Counter()\n",
    "    sizes = Counter()\n",
    "    bad_size = 0\n",
    "    non_int_labels = 0\n",
    "    multi_label = 0\n",
    "\n",
    "    for i in range(len(ds)):  # 100% du split\n",
    "        item = ds[i]\n",
    "        img = item[img_key]\n",
    "        lab = item[label_key]\n",
    "\n",
    "        if not isinstance(img, Image.Image):\n",
    "            img = Image.fromarray(img)\n",
    "\n",
    "        modes[img.mode] += 1\n",
    "        sizes[img.size] += 1\n",
    "\n",
    "        if img.size != expect_size:\n",
    "            bad_size += 1\n",
    "\n",
    "        if isinstance(lab, (list, tuple)):\n",
    "            multi_label += 1\n",
    "        elif not isinstance(lab, int):\n",
    "            non_int_labels += 1\n",
    "\n",
    "    print(f\"\\n[{split_name}]\")\n",
    "    print(f\"total = {len(ds)}\")\n",
    "    print(\"modes:\", modes)\n",
    "    print(\"sizes:\", sizes)\n",
    "    print(\"nb images taille ≠ 64x64:\", bad_size)\n",
    "    print(\"nb labels non int:\", non_int_labels)\n",
    "    print(\"nb multi-label:\", multi_label)\n",
    "\n",
    "scan_split_hf(train_hf, \"train (90k)\")\n",
    "scan_split_hf(val_hf,   \"valid (10k)\")\n",
    "scan_split_hf(test_hf,  \"test (10k)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NB PAR CLASSE (split = celui de get_dataloaders) ===\n",
      " cls |  train |    val |   test\n",
      "--------------------------------\n",
      "   0 |    450 |     50 |     50\n",
      "   1 |    450 |     50 |     50\n",
      "   2 |    450 |     50 |     50\n",
      "   3 |    450 |     50 |     50\n",
      "   4 |    450 |     50 |     50\n",
      "   5 |    450 |     50 |     50\n",
      "   6 |    450 |     50 |     50\n",
      "   7 |    450 |     50 |     50\n",
      "   8 |    450 |     50 |     50\n",
      "   9 |    450 |     50 |     50\n",
      "  10 |    450 |     50 |     50\n",
      "  11 |    450 |     50 |     50\n",
      "  12 |    450 |     50 |     50\n",
      "  13 |    450 |     50 |     50\n",
      "  14 |    450 |     50 |     50\n",
      "  15 |    450 |     50 |     50\n",
      "  16 |    450 |     50 |     50\n",
      "  17 |    450 |     50 |     50\n",
      "  18 |    450 |     50 |     50\n",
      "  19 |    450 |     50 |     50\n",
      "  20 |    450 |     50 |     50\n",
      "...\n",
      " 195 |    450 |     50 |     50\n",
      " 196 |    450 |     50 |     50\n",
      " 197 |    450 |     50 |     50\n",
      " 198 |    450 |     50 |     50\n",
      " 199 |    450 |     50 |     50\n",
      "\n",
      "Résumé :\n",
      "train total = 90000\n",
      "val   total = 10000\n",
      "test  total = 10000\n"
     ]
    }
   ],
   "source": [
    "def class_counts_hf(ds, label_key=\"label\"):\n",
    "    # HF Dataset permet souvent ds[label_key] -> liste, sinon fallback boucle\n",
    "    try:\n",
    "        labels = [int(x) for x in ds[label_key]]\n",
    "    except Exception:\n",
    "        labels = [int(ds[i][label_key]) for i in range(len(ds))]\n",
    "    return Counter(labels)\n",
    "\n",
    "train_counts = class_counts_hf(train_hf)\n",
    "val_counts   = class_counts_hf(val_hf)\n",
    "test_counts  = class_counts_hf(test_hf)\n",
    "\n",
    "num_classes = meta[\"num_classes\"]\n",
    "\n",
    "print(\"=== NB PAR CLASSE (split = celui de get_dataloaders) ===\")\n",
    "print(f\"{'cls':>4} | {'train':>6} | {'val':>6} | {'test':>6}\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "for c in range(21):\n",
    "    print(f\"{c:>4} | {train_counts[c]:>6} | {val_counts[c]:>6} | {test_counts[c]:>6}\")\n",
    "\n",
    "print(\"...\")\n",
    "\n",
    "for c in range(num_classes - 5, num_classes):\n",
    "    print(f\"{c:>4} | {train_counts[c]:>6} | {val_counts[c]:>6} | {test_counts[c]:>6}\")\n",
    "\n",
    "print(\"\\nRésumé :\")\n",
    "print(f\"train total = {len(train_hf)}\")\n",
    "print(f\"val   total = {len(val_hf)}\")\n",
    "print(f\"test  total = {len(test_hf)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "computing mean/std: 100%|██████████| 100000/100000 [01:47<00:00, 928.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: [0.4802400469779968, 0.44806626439094543, 0.39750203490257263]\n",
      "std : [0.276430606842041, 0.26886656880378723, 0.28159239888191223]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "full_train_hf = concatenate_datasets([train_hf, test_hf])  # = 100000 (le train HF d'origine)\n",
    "\n",
    "def compute_mean_std_hf(ds, desc=\"computing mean/std\", img_key=\"image\"):\n",
    "    n = len(ds)\n",
    "    sum_ = torch.zeros(3)\n",
    "    sumsq_ = torch.zeros(3)\n",
    "\n",
    "    for item in tqdm(ds, desc=desc):\n",
    "        img = item[img_key]\n",
    "        if not isinstance(img, Image.Image):\n",
    "            img = Image.fromarray(img)\n",
    "        if img.mode != \"RGB\":\n",
    "            img = img.convert(\"RGB\")\n",
    "\n",
    "        # identique à TON code\n",
    "        x = torch.tensor(list(img.getdata()), dtype=torch.float32).view(64, 64, 3)\n",
    "        x = x.permute(2, 0, 1) / 255.0  # (3,64,64)\n",
    "\n",
    "        sum_ += x.view(3, -1).mean(dim=1)\n",
    "        sumsq_ += (x.view(3, -1) ** 2).mean(dim=1)\n",
    "\n",
    "    mean = sum_ / n\n",
    "    mean_sq = sumsq_ / n\n",
    "    std = (mean_sq - mean**2).sqrt()\n",
    "\n",
    "    print(\"mean:\", mean.tolist())\n",
    "    print(\"std :\", std.tolist())\n",
    "    return mean, std\n",
    "\n",
    "_ = compute_mean_std_hf(full_train_hf, desc=\"computing mean/std\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
