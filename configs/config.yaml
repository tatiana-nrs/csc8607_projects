# Fichier de configuration générique (à adapter au projet)
dataset:
  name: zh-plus/tiny-imagenet               # ex: "CIFAR10", "IMDB", ...
  root: "./data"            # chemin local pour les données
  split:                    # optionnel : noms/tailles de splits
    train: train
    val: valid
    test: null
  download: true            # si supporté
  num_workers: 4
  shuffle: true

preprocess:
  # transformations de base (ex: resize, normalize)
  resize: [64, 64]                  # ex: [32, 32] ou null
  normalize: 
    mean: [0.48024, 0.44806, 0.39750]
    std:  [0.27643, 0.26887, 0.28159]           
  text_tokenizer: null          # pour NLP : nom ou paramètres, sinon null

augment:
  # data augmentation (laisser null si non utilisée)
  random_flip: true             # true/false
  random_crop: 
    size: [64, 64]
    scale: [0.8, 1.0]
    ratio: [0.75, 1.3333]             # paramètres ou null
  color_jitter: null            # paramètres ou null
    # brightness: 0.2
    # contrast: 0.2
    # saturation: 0.2
    # hue: 0.02
  spec_augment: null            # pour audio : paramètres ou null

model:
  type: dsconvnet                # ex: "resnet18", "mlp", "lstm", ...
  num_classes: 200
  input_shape: [3, 64, 64]             # ex: [3, 32, 32] ou null
  hidden_sizes:             # ex: [256, 128] ou null
  activation: relu          # ex: relu, gelu, tanh...
  dropout: 0.0              # ex: 0.0–0.5
  batch_norm: true          # true/false
  residual: false           # true/false
  attention: false          # true/false
  rnn:                      # pour RNN/LSTM/GRU : paramètres ou null
    type: null              # lstm/gru
    hidden_size: null
    num_layers: null
    bidirectional: false

train:
  seed: 42
  device: auto              # "cpu", "cuda", ou "auto"
  batch_size: 64
  epochs: 15
  max_steps: null           # entier ou null
  overfit_small: false      # true pour sur-apprendre sur un petit échantillon

  optimizer:
    name: adam              # sgd/adam/rmsprop
    lr: 0.001
    weight_decay: 0.0
    # momentum: 0.9           # utile si SGD

  scheduler:
    name: none              # none/step/cosine/onecycle
    step_size: 10
    gamma: 0.1
    warmup_steps: 0

metrics:
  classification:           # ex: ["accuracy", "f1"]
    - accuracy
  regression: []            # ex: ["mae", "rmse"]

hparams:                    # espace pour mini grid search
  lr: [1e-4, 3e-4, 1e-3, 3e-3, 1e-2]
  batch_size: [64, 128]
  weight_decay: [0.0, 1e-5, 1e-4, 1e-3]
  B: [2,2,2]
    # - [1,1,1]
  width: 1.0


paths:
  runs_dir: "./runs"
  artifacts_dir: "./artifacts"